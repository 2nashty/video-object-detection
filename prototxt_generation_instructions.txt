If you want to pretrain a model on an existing model, follow the 
instructions on the flickr-style finetuning example:
https://github.com/BVLC/caffe/tree/master/examples/finetune_flickr_style
To avoid a loss of nan, change the weight_filler types from gaussian to xavier,
and make sure the base learning rate is 0.001 or lower.

If you want to train a model from scratch, follow these instructions:
After completeing these instructions, you will have made these 3 files:
train_val.protxt, solver.protxt, and deploy.prototxt

Use absolute paths for simplicity.

First, copy the model's train_val.prototxt to
data/imagenet/<wnid_dir>/images/<dataset>/aux
Change the files for each "source:" and "mean_file:" line to the lmbd
files and image mean created by prepare_data.py.
Find the last layer that contains a "num_output" field and change that to
the number of categories (in my case, two: one positive, one negative).
I also change the batch_size to 64, in case I run out of memory
on my macbook.

Copy the model's solver.prototxt to
data/imagenet/<wnid_dir>/images/<dataset>/aux
Change the "net:" path to the path to the train_val.prototxt you just created.
Change the "snapshot_prefix:" to
data/imagenet/<wnid_dir>/snapshots/images/<dataset>/snapshots
Change the "server_mode:" to CPU
Change the "snapshot:" to 1000. On my macbook, it takes 3 minutes per 20
iterations, so this produces one snapshot per half hour. On a K40 machine
training the bvlc_reference_caffenet, training the
bvlc_refference_caffenet on 1000 ImageNet categories takes 26 seconds per
20 iterations, which is ~5.2 ms per image, so I think they're training on about
5100 images. I'm training on 19500 images (there are 7 times as many
negative images as positive images)
Change the "max_iter" to 3000. (this is 45000 for NIN, but that would take
me 112 hours, which is 4.7 days). This is the same as test_iter. It's
annoying that the test is also performed on an untrained network, because
that takes 70 minutes, and just tells us the ratio of our positive to negative
images.

Copy this to a new file called deploy.prototxt:

```
name: "<the name in the train_val.prototxt>"
input: "data"
input_dim: 10
input_dim: 3
input_dim: 224
input_dim: 224
```

Append to that all layers in train_val.prototxt.
Delete the first few layers that don't have a "bottom" field.
Delete all pramaters that have to do exclusively with learning.
e.g.:
  - blobs_lr
  - weight_decay
  - weight_filler
  - bias_filler
Delete the "accuracy" layer and any layer after it (probably the softmax)
Append to the file this final layer:

```
# R-CNN classification layer made from R-CNN ILSVRC13 SVMs.
layers {
  name: "fc-rcnn"
  type: INNER_PRODUCT
  bottom: <name of layer right above this one>
  top: "fc-rcnn"
  inner_product_param {
    num_output: <change this to the number of categories>
  }
}
```
In the last layer before the R-CNN layer that contains a "num_output" field,
change that value to the number of categories (in my case, 2).

References for generating deploy.prototxt:
  https://github.com/BVLC/caffe/issues/1245
  https://github.com/BVLC/caffe/issues/261
