
Use absolute paths for simplicity.

First, copy the model's train_val.prototxt to data/imagenet/<wnid_dir>/aux
Change the files for each "source:" and "mean_file:" line to the lmbd
files and image mean created by prepare_data.py.
Find the last layer that contains a "num_output" field and change that to
the number of categories (in my case, two: one positive, one negative).
Maybe also change the batch_size from 64 to 32, in case I run out of memory
on my macbook.

Copy the model's solver.prototxt to data/imagenet/<wnid_dir>/aux
Change the "net:" path to the path to the train_val.prototxt you just created.
Change the "snapshot_prefix:" to data/imagenet/<wnid_dir>/snapshots
Change the "sorver_mode:" to CPU

The deploy.prototxt appears to be difficult to generate. NIN unfortunately
doesn't provide one. See https://github.com/BVLC/caffe/issues/1245
and https://github.com/BVLC/caffe/issues/261

Copy this to a new file called deploy.prototxt:

```
name: "<the name from train_val.prototxt>"
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
```

Append to that all layers in train_val.prototxt.
Delete the first few layers that don't have a "bottom" field.
Delete all pramaters that have to do exclusively with learning.
e.g.:
  - blobs_lr
  - weight_decay
  - weight_filler
  - bias_filler
Delete the "accuracy" layer and any layer after it (probably the softmax)
Append to the file this final layer:

```
# R-CNN classification layer made from R-CNN ILSVRC13 SVMs.
layers {
  name: "fc-rcnn"
  type: INNER_PRODUCT
  bottom: "fc7"
  top: "fc-rcnn"
  inner_product_param {
    num_output: <change this to the number of categories>
  }
}
```
